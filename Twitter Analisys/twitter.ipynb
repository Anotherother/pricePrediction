{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tweepy as TW\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "# connect to twitter API\n",
    "\n",
    "CONSUMER_KEY = 'XkmU3czKdJ1jfzUAIWELFC9y8'\n",
    "CONSUMER_SECRET = 'HlxpwfuShIh8OXRGjWgT2h3fD9JMC6HNtQ11N28WFFDzpCUHO0'\n",
    "ACCESS_TOKEN = \t'4826647649-GRcsvzCoPjdTdosCCgYY9zalhBytNbSuVaemIvH'\n",
    "ACCESS_TOKEN_SECRET = '93l4Mh1KLfexPNxwqZ4mtjUhseGWe5xQecS0Sd7YfdCbm'\n",
    "\n",
    "auth =TW.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n",
    "auth.set_access_token(ACCESS_TOKEN, ACCESS_TOKEN_SECRET)  \n",
    "api = TW.API(auth)\n",
    "\n",
    "# Load data in Dataframe with Twitter API\n",
    "data = {'date': [], 'text': []}\n",
    "\n",
    "for tweet in TW.Cursor(api.search, \n",
    "                    q=\"blockchain\",                       \n",
    "                    since=\"2017-06-10\", \n",
    "                    until=\"2017-08-10\", \n",
    "                    lang=\"en\").items(100):   \n",
    "    \n",
    "        #csvWriter.writerow([tweet.created_at, tweet.text.encode('utf-8')])\n",
    "        data['text'].append( tweet.text)\n",
    "        data['date'].append(tweet.created_at)\n",
    "#Оберточка дикт в пандас\n",
    "data = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data.text = data.text.drop_duplicates()\n",
    "data = data.dropna()\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_list = nltk.corpus.stopwords.words('english') + [\"rt\"] + ['[slides]']+ [\"re\"]\n",
    "\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "reg = '(\\:\\w+\\:|\\<[\\/\\\\]?3|[\\(\\)\\\\\\D|\\*\\$][\\-\\^]?[\\:\\;\\=]|[\\:\\;\\=B8][\\-\\^]?[3DOPp\\@\\$\\*\\\\\\)\\(\\/\\|])(?=\\s|[\\!\\.\\?]|$)'\n",
    "\n",
    "emoji_pattern = re.compile(u'('\n",
    "    u'\\ud83c[\\udf00-\\udfff]|'\n",
    "    u'\\ud83d[\\udc00-\\ude4f\\ude80-\\udeff]|'\n",
    "    u'[\\u2600-\\u26FF\\u2700-\\u27BF])+', \n",
    "    re.UNICODE)\n",
    "\n",
    "classifier =[]\n",
    "def preprocess(tweet):   \n",
    "\n",
    "    tweet = tweet.lower()\n",
    "    #tweet = \" \".join(tweet.split('#'))\n",
    "    tweet = re.sub('((www\\.[^\\s]+)|(https://[^\\s]+))','',tweet)\n",
    "    tweet = re.sub('((pic\\.[^\\s]+)|(https://[^\\s]+))','',tweet)\n",
    "    tweet = re.sub(\"(http\\S+)|(https\\S+)\", '', tweet)\n",
    "    tweet = re.sub(reg,'',tweet)\n",
    "    #tweet = re.sub(u'[a-zA-Z0-9./]+\\.[a-zA-Z0-9./ ]+.*$','',tweet)\n",
    "    \n",
    "    tweet = re.sub(u'[\\(\\)]',' ',tweet)\n",
    "    tweet = re.sub(u'\\n',' ',tweet)\n",
    "    tweet = re.sub(u'@[^\\s]+','',tweet)  \n",
    "    tweet = re.sub(r'#[^\\s]+', '', tweet)\n",
    "    tweet = tweet.strip('\\'\"')\n",
    "    twitter = emoji_pattern.sub('', tweet)\n",
    "    tweet = re.sub('[\\W]', ' ', tweet)\n",
    "    \n",
    "    # Removing stop words \n",
    "    tweet  = \" \".join([word for word in tweet.split(\" \") if word not in stop_list])\n",
    "    tweet = \" \".join([lemmatizer.lemmatize(word) for word in tweet.split(\" \")])\n",
    "\n",
    "\n",
    "    return tweet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['processed_text'] = data.text.apply(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic analise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "s_analisys_dataframe = pd.DataFrame()\n",
    "\n",
    "for i in data.index:\n",
    "    ss = sid.polarity_scores(data.processed_text[i])\n",
    "    ss['date'] = data.date[i]\n",
    "    returnDF = pd.DataFrame(pd.Series(ss,index=ss.keys())).T\n",
    "    s_analisys_dataframe = s_analisys_dataframe.append(returnDF, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
